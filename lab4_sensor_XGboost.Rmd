```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
There are 3-4 packages you will need to install for today's practical: `install.packages(c("xgboost", "eegkit", "forecast", "tseries", "caret"))` apart from that everything else should already be available on your system. 

If you are using a newer Mac you may have to also install [quartz](https://www.xquartz.org/) to have everything work (do this if you see errors about `X11` during install/execution).

I will endeavour to use explicit imports to make it clear where functions are coming from (functions without `library_name::` are part of base R or a function we've defined in this notebook).


```{r libraries, echo=FALSE}
# Using the same library we used earlier in the course for tabular data because we know it works!
library(xgboost)

# EEG manipulation library in R (although very limited compared to signal processing libraries available in other languages, matlab might actually still be a leader in this specific area)
library(eegkit)

# some time series functions (that we only skim the depths of)
library(forecast)
library(tseries)
library(caret)

# just tidyverse libraries that should already be installed
library(dplyr)
library(reshape2)
library(purrr)
library(ggplot2)
```

## EEG Eye Detection Data

One of the most common types of medical sensor data (and one that we talked about during the lecture) are Electroencephalograms (EEGs).  
These measure mesoscale electrical signals (measured in microvolts) within the brain, which are indicative of a region of neuronal activity.
Typically, EEGs involve an array of sensors (aka channels) placed on the scalp with a high degree of covariance between sensors.

As EEG data can be very large and unwieldy, we are going to use a relatively small/simple dataset today from [this paper](http://ehrai.com/su/pdf/aihls2013.pdf).

This dataset is a 117 second continuous EEG measurement collected from a single person with a device called a "Emotiv EEG Neuroheadset".
In combination with the EEG data collection, a camera was used to record whether person being recorded had their eyes open or closed. 
This was eye status was then manually annotated onto the EEG data with `1` indicated the eyes being closed and `0` the eyes being open.
Measures microvoltages are listed in chronological order with the first measured value at the top of the dataframe.

Let's parse the data directly from the `h2o` library's (which we aren't actually using directly) test data S3 bucket:

```{r parse_data}
eeg_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/eeg/eeg_eyestate_splits.csv"
eeg_data <- read.csv(eeg_url)

# add timestamp
Fs <- 117 / nrow(eeg_data)
eeg_data <- transform(eeg_data, ds = seq(0, 116.99999, by = Fs), eyeDetection = as.factor(eyeDetection))
print(table(eeg_data$eyeDetection))

# split dataset into train, validate, test
eeg_train <- subset(eeg_data, split == 'train', select = -split)
print(table(eeg_train$eyeDetection))

eeg_validate <- subset(eeg_data, split == 'valid', select = -split)
eeg_test <- subset(eeg_data, split == 'test', select = -split)
```

**0** Knowing the `eeg_data` contains 117 seconds of data, inspect the `eeg_data` dataframe and the code above to and determine how many samples per second were taken?

```{r}
nrow(eeg_data)

```
So, the total number of samples is 14980

Therefore, the number of samples per second is: 14980/117 ~ 128

Thus, there are approximately 128 samples per second.


**1** How many EEG electrodes/sensors were used?

To determine the number of EEG electrodes/sensors used, we need to examine the columns in the eeg_data dataframe. The columns represent different attributes recorded, including the EEG sensors and metadata (like split, eyeDetection, and ds).

By looking at the names of the columns (excluding metadata columns like split, eyeDetection, and ds), we can determine the number of EEG sensors.

```{r}
colnames(eeg_data)

```
If we ignore attributes "eyeDetection", "split", and "ds", there will be a total of 14 EEG electrodes/sensors that were used.

### Exploratory Data Analysis

Now that we have the dataset and some basic parameters let's begin with the ever important/relevant exploratory data analysis.

First we should check there is no missing data!
```{r check_na}
sum(is.na(eeg_data))
```

Great, now we can start generating some plots to look at this data within the time-domain.

First we use `reshape2::melt()` to transform the `eeg_data` dataset from a wide format to a long format expected by `ggplot2`.

Specifically, this converts from "wide" where each electrode has its own column, to a "long" format, where each observation has its own row. 
This format is often more convenient for data analysis and visualization, especially when dealing with repeated measurements or time-series data.

We then use `ggplot2` to create a line plot of electrode intensities per sampling time, with the lines coloured by electrode, and the eye status annotated using dark grey blocks.

```{r plot_data}
melt <- reshape2::melt(eeg_data %>% dplyr::select(-split), id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")


ggplot2::ggplot(melt, ggplot2::aes(x=ds, y=microvolts, color=Electrode)) + 
  ggplot2::geom_line() + 
  ggplot2::ylim(3500,5000) + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(melt, eyeDetection==1), alpha=0.005)
```

**2** Do you see any obvious patterns between eyes being open (dark grey blocks in the plot) and the EEG intensities?

Yes, there are noticeable patterns:

When the eyes are closed (shown by dark grey blocks), the EEG signals tend to be more variable and show more pronounced changes.
The signals seem to be more active and synchronized during these periods compared to when the eyes are open.

**3** Similarly, based on the distribution of eye open/close state over time to anticipate any temporal correlation between these states?

The eyes' condition—whether they are closed or open—seems to depend in part on earlier stages. For instance, there may be a greater likelihood of going from an open state to a closed state and vice versa after an open time.
A type of temporal correlation may be suggested by underlying temporal patterns or cycles (such as regular intervals of eye blinks or resting periods).
Further statistical study, such as determining autocorrelation or performing time-series analysis to quantify these dependencies, would be required in order to firmly demonstrate these temporal relationships.

Let's see if we can directly look at the distribution of EEG intensities and see how they related to eye status.


As there are a few extreme outliers in voltage we will use the `dplyr::filter` function to remove values outwith of 3750 to 50003. The function uses the `%in%` operator to check if each value of microvolts is within that range. The function also uses the `dplyr::mutate()` to change the type of the variable eyeDetection from numeric to a factor (R's categorical variable type).

```{r compare_distrib}
melt_train <- reshape2::melt(eeg_train, id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")

# filter huge outliers in voltage
filt_melt_train <- dplyr::filter(melt_train, microvolts %in% (3750:5000)) %>% dplyr::mutate(eyeDetection=as.factor(eyeDetection))

ggplot2::ggplot(filt_melt_train, ggplot2::aes(y=Electrode, x=microvolts, fill=eyeDetection)) + ggplot2::geom_boxplot()
```



Plots are great but sometimes so it is also useful to directly look at the summary statistics and how they related to eye status.
We will do this by grouping the data based on eye status and electrode before calculating the statistics using the convenient `dplyr::summarise` function.

```{r compare_summary_stats}
filt_melt_train %>% dplyr::group_by(eyeDetection, Electrode) %>% 
    dplyr::summarise(mean = mean(microvolts), median=median(microvolts), sd=sd(microvolts)) %>% 
    dplyr::arrange(Electrode)
```




**4** Based on these analyses are any electrodes consistently more intense or varied when eyes are open?

Here are the key points from the summary statistics:

AF3 Electrode:

Eyes open (0): mean = 4294, median = 4300, sd = 35.4
Eyes closed (1): mean = 4305, median = 4300, sd = 34.4
Observation: The mean is slightly higher when eyes are closed, but the standard deviation (variation) is similar.
F7 Electrode:

Eyes open (0): mean = 4015, median = 4020, sd = 28.4
Eyes closed (1): mean = 4007, median = 4000, sd = 24.9
Observation: The mean is slightly higher when eyes are open, but the variation is somewhat higher when eyes are open.
F3 Electrode:

Eyes open (0): mean = 4268, median = 4260, sd = 20.9
Eyes closed (1): mean = 4269, median = 4260, sd = 17.4
Observation: The mean and median are almost the same for both states, with slightly more variation when eyes are open.
FC5 Electrode:

Eyes open (0): mean = 4124, median = 4120, sd = 17.3
Eyes closed (1): mean = 4124, median = 4120, sd = 19.2
Observation: The mean and median are the same for both states, with slightly more variation when eyes are closed.
T7 Electrode:

Eyes open (0): mean = 4341, median = 4340, sd = 13.9
Eyes closed (1): mean = 4342, median = 4340, sd = 15.5
Observation: The mean and median are almost the same for both states, with slightly more variation when eyes are closed.
Overall Observation:
Intensity: No electrode shows a consistently higher intensity when the eyes are open across all electrodes. The mean and median values are very similar for both states in most electrodes, with some showing slightly higher means when eyes are closed (e.g., AF3).
Variation: The variation (standard deviation) does not show a consistent pattern across all electrodes. Some electrodes (like F7) show slightly more variation when eyes are open, while others (like FC5 and T7) show more variation when eyes are closed.
Therefore, based on these analyses, there isn't a clear and consistent pattern indicating that any particular electrode is more intense or varied when the eyes are open compared to when they are closed. The differences in intensity and variation are relatively small and not consistent across all electrodes.

#### Time-Related Trends

As it looks like there may be a temporal pattern in the data we should investigate how it changes over time.  

First we will do a statistical test for stationarity:

```{r convert_to_tseries}
apply(eeg_train, 2, tseries::adf.test)
```


**5** What is stationarity?

Stationarity in a time series means that its key statistical properties (like mean, variance, and autocorrelation) do not change over time. Simply put, if we look at the data at any point, it behaves the same way throughout the entire period.

**6** Why are we interested in stationarity? What do the results of these tests tell us? (ignoring the lack of multiple comparison correction...)

We are interested in stationarity because many time-series analysis methods, including forecasting models, work best with stationary data. If the data is not stationary, the models might give unreliable or inaccurate results.

Results of the ADF tests:

Stationary Variables (p-value = 0.01): AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4, eyeDetection
These variables have consistent statistical properties over time, making them suitable for many time-series analysis methods.

Non-Stationary Variable (p-value = 0.4045): ds
This variable changes its statistical properties over time, indicating trends or patterns. It is not stable and might need transformation (like differencing) to make it stationary before further analysis.

In conclusion, the time-related variable ds is not stable across time, although the majority of the EEG electrode measurements and the eyeDetection variable are.

Then, using the {forecast::ggAcf} function, we might want to visually investigate patterns of autocorrelation—where past values predict future ones—and cross-correlation—where correlation across channels changes over time.

The cross- and auto-correlation values for various lags—that is, time-delayed versions of the voltage timeseries for each electrode in the dataset—are shown in the ACF plot. 
It aids in locating any noteworthy relationships between channels and data at various intervals. 
When there is positive autocorrelation, the voltage increase that is seen at a particular time interval also causes a corresponding increase in the lag time interval.
Negative autocorrelation indicates the opposite!


```{r correlation}
forecast::ggAcf(eeg_train %>% dplyr::select(-ds))
```





**7** Do any fields show signs of strong autocorrelation (diagonal plots)? Do any pairs of fields show signs of cross-correlation? Provide examples.

Autocorrelation (diagonal plots):

Strong Autocorrelation: The diagonal plots represent the autocorrelation of each electrode over different lags. In the ACF plot, strong autocorrelation is indicated by bars that extend significantly beyond the blue dashed lines.
Examples of Strong Autocorrelation:
AF3: The diagonal plot for AF3 shows significant bars at various lags, indicating strong autocorrelation.
F3: Similarly, F3 also displays significant autocorrelation.
P7 and O2: Both of these electrodes exhibit strong autocorrelation across several lags.

Cross-Correlation (off-diagonal plots):

Strong Cross-Correlation: The off-diagonal plots show the correlation between different electrodes over various lags. Strong cross-correlation is observed when bars extend beyond the blue dashed lines in these plots.
Examples of Cross-Correlation:
AF3 and F7: The plot showing the correlation between AF3 and F7 has significant bars, indicating a strong cross-correlation.
F3 and FC5: These electrodes also show signs of cross-correlation as indicated by the significant bars in their plot.
P7 and O1: The cross-correlation between P7 and O1 is noticeable with significant bars, suggesting a relationship over time.
Summary:

Autocorrelation: AF3, F3, P7, and O2 show strong autocorrelation.
Cross-Correlation: Pairs like AF3 & F7, F3 & FC5, and P7 & O1 demonstrate strong cross-correlation.


#### Frequency-Space 

We can also explore the data in frequency space by using a Fast Fourier Transform.  
After the FFT we can summarise the distributions of frequencies by their density across the power spectrum.
This will let us see if there any obvious patterns related to eye status in the overall frequency distributions.

```{r fft_open}
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 0) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Open")
```

```{r fft_closed}
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 1) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Closed")
```




**8** Do you see any differences between the power spectral densities for the two eye states? If so, describe them.

Indeed, there are noticeable changes between the two open and closed eye states' power spectral densities (PSDs).
In Condition 1 (Open Eyes):

Higher power is concentrated in the lower frequency region (about 0.001 Hz) according to the PSD.
Higher power density values in that frequency range are indicated by the colour pattern, which is primarily blue and green.

When in State 2 (Closed Eye):

When compared to the open eye state, the PSD exhibits reduced power over the whole frequency range.
The lower frequencies (around 0.001 Hz) have a noticeable orange and yellow band surrounding them, which suggests that the power density there is larger than it is in the other frequencies.


#### Independent Component Analysis

We may also wish to explore whether there are multiple sources of neuronal activity being picked up by the sensors.  
This can be achieved using a process known as independent component analysis (ICA) which decorrelates the channels and identifies the primary sources of signal within the decorrelated matrix.

```{r ica, warning=FALSE}
ica <- eegkit::eegica(eeg_train %>% dplyr::select(-eyeDetection, -ds), nc=3, method='fast', type='time')
mix <- dplyr::as_tibble(ica$M)
mix$eyeDetection <- eeg_train$eyeDetection
mix$ds <- eeg_train$ds

mix_melt <- reshape2::melt(mix, id.vars=c("eyeDetection", "ds"), variable.name = "Independent Component", value.name = "M")


ggplot2::ggplot(mix_melt, ggplot2::aes(x=ds, y=M, color=`Independent Component`)) + 
  ggplot2::geom_line() + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(mix_melt, eyeDetection==1), alpha=0.005) +
  ggplot2::scale_y_log10()
```



**9** Does this suggest eye opening relates to an independent component of activity across the electrodes?

The independent components (v1, v2, and v3) throughout the time points are displayed in the analysis above, and it suggests that the data contains unique patterns or sources of activity. At some time intervals, the red component (v1) exhibits noticeable peaks or spikes that suggest bursts of activity that might be connected to eye-opening events.

The existence of these independent sources of activity across the electrodes implies that eye opening may be connected to one or more of these components, even though we are unable to definitively prove a link between the independent components and eye opening. In particular, the red component (v1) is a likely contender to capture the fleeting activity linked to eye opening events due to its spiky structure.

As a result, the research raises the possibility that eye opening is represented in a separate component of activity across the electrodes, maybe the red component (v1) due to its unique spiking pattern, without drawing any firm conclusions.


### Eye Opening Prediction

Now that we've explored the data let's use a simple model to see how well we can predict eye status from the EEGs:

```{r xgboost}
# Convert the training and validation datasets to matrices
eeg_train_matrix <- as.matrix(dplyr::select(eeg_train, -eyeDetection, -ds))
eeg_train_labels <- as.numeric(eeg_train$eyeDetection) -1

eeg_validate_matrix <- as.matrix(dplyr::select(eeg_validate, -eyeDetection, -ds))
eeg_validate_labels <- as.numeric(eeg_validate$eyeDetection) -1

# Build the xgboost model
model <- xgboost(data = eeg_train_matrix, 
                 label = eeg_train_labels,
                 nrounds = 100,
                 max_depth = 4,
                 eta = 0.1,
                 objective = "binary:logistic")

print(model)
```



**10** Using the `caret` library (or any other library/model type you want such as a naive Bayes) fit another model to predict eye opening.

Let's use the caret library to fit another model for predicting eye opening. We'll use a simple logistic regression model.

```{r model2}

library(caret)
library(dplyr)

# Lets prepare the training and validation datasets
eeg_train_matrix <- as.matrix(dplyr::select(eeg_train, -eyeDetection, -ds))
eeg_train_labels <- as.factor(eeg_train$eyeDetection)

eeg_validate_matrix <- as.matrix(dplyr::select(eeg_validate, -eyeDetection, -ds))
eeg_validate_labels <- as.factor(eeg_validate$eyeDetection)

# Let us combine the training data into a single data frame
train_data <- data.frame(eeg_train_matrix, eyeDetection = eeg_train_labels)
validate_data <- data.frame(eeg_validate_matrix, eyeDetection = eeg_validate_labels)

# Logistic regression model
set.seed(123)
logistic_model <- train(eyeDetection ~ ., data = train_data, method = "glm", family = "binomial")

logistic_predictions <- predict(logistic_model, newdata = validate_data)

confusionMatrix(logistic_predictions, validate_data$eyeDetection)

```


**11** Using the best performing of the two models (on the validation dataset) calculate and report the test performance (filling in the code below):

```{r test}

# Lets convert the test dataset to a matrix
eeg_test_matrix <- as.matrix(dplyr::select(eeg_test, -eyeDetection, -ds))
eeg_test_labels <- as.numeric(eeg_test$eyeDetection) - 1

# Predictions using the XGBoost model
test_predictions <- predict(model, eeg_test_matrix)
test_predictions_class <- ifelse(test_predictions > 0.5, 1, 0)

# Now lets evaluate the performance on the test dataset
library(caret)
confusion_matrix <- confusionMatrix(as.factor(test_predictions_class), as.factor(eeg_test_labels))

# Print the confusion matrix
print(confusion_matrix)

```

**12** Describe 2 possible alternative modeling approaches for prediction of eye opening from EEGs we discussed in the lecture but haven't explored in this notebook.

Markov's Hidden Model (HMM)
What is it: Because HMM is a statistical model that works with data sequences, it can be used to analyse time-series data, such as EEG signals.
How Operates: The model is based on the assumption that the system is a Markov process with hidden states. In the context of EEG data, this means that the observed EEG signals are produced by a series of hidden states that represent the brain activity.
Why It's Beneficial: By recognising particular patterns in the brain waves, HMM may identify temporal patterns and transitions in EEG data, which can aid in the prediction of eye-opening occurrences.

Neural Network
What It Is: Inspired by the human brain, neural networks are a class of machine learning models that can recognise intricate patterns in data.
How Operates: They are made up of layers of networked nodes, or neurons. After processing the input data, each layer transfers it to the subsequent layer. The network can be trained to identify patterns in the EEG data linked to eye-opening experiences in order to predict EEG activity.
Why It's Beneficial: Large volumes of data may be handled by neural networks, and deep learning models in particular are capable of learning complex patterns. When trained with sufficient EEG data, they can be quite effective in generating precise predictions.


**13** What are 2 R libraries you could use to implement these approaches? (note: you don't actually have to implement them though!)

Library for the Hidden Markov Model (HMM): depmixS4
Functions for fitting Hidden Markov Models to time-series data are included in this package. Along with supporting multivariate and univariate data, it provides a range of model fitting and prediction techniques.
Example: Based on the learnt hidden states, we can utilise depmixS4 to generate and fit an HMM to our EEG data, which will aid in the prediction of eye-opening events..

Library for Neural Network: keras
The R interface for the Keras deep learning library is called keras. It makes it simple to construct, train, and assess neural networks. Keras is a high-level API for neural network modelling that is developed on top of TensorFlow.
Example: By identifying patterns in the brain signals, we can use Keras to create a neural network model, train it on our EEG data, and use it to forecast eye-opening events.

## Optional

**14** (Optional) As this is the last practical of the course - let me know how you would change future offerings of this course. This will not impact your marks!

- What worked and didn’t work for you (e.g., in terms of the practicals, tutorials, and lectures)?

- Was learning how to run the practicals on your own machines instead of a clean server that will disappear after the course worth the technical challenges?
 
- What would you add or remove from the course? 

- What was the main thing you will take away from this course?